{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "from scipy.sparse import csr_matrix \n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 most common words: [('the', 69971), ('of', 36412), ('and', 28853), ('to', 26158), ('a', 23195)]\n",
      "top 5 least common words: [('vertex', 19), ('rourke', 19), ('killpath', 19), ('haney', 19), ('letch', 19)]\n"
     ]
    }
   ],
   "source": [
    "#Pre-exercise\n",
    " \n",
    "words = brown.words()\n",
    "\n",
    "words = [w.lower() for w in words if w.isalpha()]\n",
    "\n",
    "c = Counter(words)\n",
    "top_5k = c.most_common(5000)\n",
    "print(f\"top 5 most common words: {top_5k[:5]}\")\n",
    "print(f\"top 5 least common words: {top_5k[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of new words added: 30\n"
     ]
    }
   ],
   "source": [
    "# load in words from table 1\n",
    "r = requests.get('https://raw.githubusercontent.com/AlexGrinch/ro_sgns/master/datasets/rg65.csv')\n",
    "rg65_set = set([word for line in r.text.split('\\n') for word in line.strip().split(';')[:2]])\n",
    "\n",
    "top_5k_words_initial = [i[0] for i in top_5k]\n",
    "top_5k_words_after = [i[0] for i in top_5k]\n",
    "new_words = []\n",
    "for word in rg65_set:\n",
    "    if word not in top_5k_words_initial:\n",
    "        top_5k_words_after.append(word)\n",
    "        new_words.append(word)\n",
    "\n",
    "print(f\"number of new words added: {len(new_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbr of bigrams that matched the search: 754420\n"
     ]
    }
   ],
   "source": [
    "#get bigrams\n",
    "\n",
    "bigrams = ngrams(words, 2)\n",
    "top_5k_bigrams = []\n",
    "\n",
    "for gram in bigrams:\n",
    "    if gram[0] in top_5k_words_after and gram[1] in top_5k_words_after:\n",
    "        top_5k_bigrams.append(gram)\n",
    "\n",
    "bigrams_freq = Counter(top_5k_bigrams)\n",
    "print(f\"numbr of bigrams that matched the search: {len(top_5k_bigrams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create M1 matrix\n",
    "M1 = csr_matrix((len(top_5k_words_after), len(top_5k_words_after)),  \n",
    "                          dtype = np.int32).toarray()\n",
    "\n",
    "index_dict = {k: v for v, k in enumerate(top_5k_words_after)}\n",
    "\n",
    "for entry in bigrams_freq:\n",
    "    row = entry[0]\n",
    "    col = entry[1]\n",
    "    count = bigrams_freq[entry]   \n",
    "    \n",
    "    M1[index_dict[row], index_dict[col]] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppmi on M1\n",
    "def ppmi(mat):\n",
    "    col_totals = mat.sum(axis=0)\n",
    "    total = col_totals.sum()\n",
    "    row_totals = mat.sum(axis=1)\n",
    "    expected = np.outer(row_totals, col_totals) / total\n",
    "    mat = mat / (expected + 1e-16)\n",
    "    # Silence distracting warnings about log(0):\n",
    "    with np.errstate(divide='ignore'):\n",
    "        mat = np.log(mat)\n",
    "    mat[np.isinf(mat)] = 0.0  # log(0) = 0\n",
    "    mat[mat < 0] = 0.0\n",
    "    return mat\n",
    "\n",
    "M1_plus = ppmi(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca10 = PCA(n_components=10)\n",
    "pca100 = PCA(n_components=100)\n",
    "pca300 = PCA(n_components=300)\n",
    "\n",
    "M2_10 = pca10.fit_transform(M1_plus)\n",
    "M2_100 = pca100.fit_transform(M1_plus)\n",
    "M2_300 = pca300.fit_transform(M1_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs of words \n",
    "rg65 = [s.strip().split(';') for s in r.text.split('\\n')]\n",
    "\n",
    "P = []\n",
    "S = []\n",
    "for w1, w2, score in rg65:\n",
    "    if w1 in new_words or w2 in new_words:\n",
    "        P.append((w1, w2))\n",
    "        S.append(float(score))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [M1, M1_plus, M2_10, M2_100, M2_300]\n",
    "M1_a, M1_plus_a, M2_10_a, M2_100_a, M2_300_a = [], [], [], [], []\n",
    "arrs = [M1_a, M1_plus_a, M2_10_a, M2_100_a, M2_300_a]\n",
    "for i, model in enumerate(models):\n",
    "    for w1, w2, in P:\n",
    "        arrs[i].append(metrics.pairwise.cosine_similarity(model[index_dict[w1]].reshape(1, -1), model[index_dict[w2]].reshape(1, -1))[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson stats for model M1: 0.23682594116301078\n",
      "pearson stats for model M1_plus: 0.201237999273438\n",
      "pearson stats for model M2_10: 0.24786070380823913\n",
      "pearson stats for model M2_100: 0.41870300892924095\n",
      "pearson stats for model M2_300: 0.39571575881845256\n"
     ]
    }
   ],
   "source": [
    "#pearson\n",
    "model_names = ['M1', 'M1_plus', 'M2_10', 'M2_100', 'M2_300']\n",
    "for i, arr in enumerate(arrs):\n",
    "    pearson = scipy.stats.pearsonr(arr, S)\n",
    "    print(f\"pearson stats for model {model_names[i]}: {pearson[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cord', 'smile'), ('rooster', 'voyage'), ('fruit', 'furnace'), ('autograph', 'shore'), ('automobile', 'wizard'), ('mound', 'stove'), ('grin', 'implement'), ('asylum', 'fruit'), ('asylum', 'monk'), ('graveyard', 'madhouse'), ('glass', 'magician'), ('boy', 'rooster'), ('cushion', 'jewel'), ('monk', 'slave'), ('asylum', 'cemetery'), ('grin', 'lad'), ('shore', 'woodland'), ('monk', 'oracle'), ('boy', 'sage'), ('automobile', 'cushion'), ('mound', 'shore'), ('lad', 'wizard'), ('forest', 'graveyard'), ('food', 'rooster'), ('cemetery', 'woodland'), ('shore', 'voyage'), ('bird', 'woodland'), ('furnace', 'implement'), ('crane', 'rooster'), ('hill', 'woodland'), ('cemetery', 'mound'), ('glass', 'jewel'), ('magician', 'oracle'), ('crane', 'implement'), ('brother', 'lad'), ('sage', 'wizard'), ('oracle', 'sage'), ('bird', 'crane'), ('bird', 'cock'), ('brother', 'monk'), ('asylum', 'madhouse'), ('furnace', 'stove'), ('magician', 'wizard'), ('hill', 'mound'), ('cord', 'string'), ('glass', 'tumbler'), ('grin', 'smile'), ('serf', 'slave'), ('journey', 'voyage'), ('autograph', 'signature'), ('forest', 'woodland'), ('implement', 'tool'), ('cock', 'rooster'), ('boy', 'lad'), ('cushion', 'pillow'), ('cemetery', 'graveyard'), ('midday', 'noon'), ('gem', 'jewel')]\n",
      "[0.02, 0.04, 0.05, 0.06, 0.11, 0.14, 0.18, 0.19, 0.39, 0.42, 0.44, 0.44, 0.45, 0.57, 0.79, 0.88, 0.9, 0.91, 0.96, 0.97, 0.97, 0.99, 1.0, 1.09, 1.18, 1.22, 1.24, 1.37, 1.41, 1.48, 1.69, 1.78, 1.82, 2.37, 2.41, 2.46, 2.61, 2.63, 2.63, 2.74, 3.04, 3.11, 3.21, 3.29, 3.41, 3.45, 3.46, 3.46, 3.58, 3.59, 3.65, 3.66, 3.68, 3.82, 3.84, 3.88, 3.94, 3.94]\n"
     ]
    }
   ],
   "source": [
    "# get P and S from table 1\n",
    "rg65 = [s.strip().split(';') for s in r.text.split('\\n')]\n",
    "\n",
    "P = []\n",
    "S = []\n",
    "for w1, w2, score in rg65:\n",
    "    if w1 in new_words or w2 in new_words:\n",
    "        P.append((w1, w2))\n",
    "        S.append(float(score))\n",
    "\n",
    "print(P)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sims = []\n",
    "\n",
    "for w1, w2 in P:\n",
    "    cosine_sims.append(metrics.pairwise.cosine_similarity(model[w1].reshape(1, -1), model[w2].reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson correlation between w2v and human similarities 0.7516600628757987\n"
     ]
    }
   ],
   "source": [
    "pearson = scipy.stats.pearsonr(cosine_sims, S)\n",
    "print(f\"pearson correlation between w2v and human similarities {pearson[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create analogies dictionary\n",
    "\n",
    "r = requests.get('http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt')\n",
    "\n",
    "analogies_dict = {}\n",
    "current_collection = ''\n",
    "for line in r.text.split(\"\\n\")[1:]:\n",
    "    if line.startswith(\":\"):\n",
    "        current_collection = line[1:].strip()\n",
    "        analogies_dict[current_collection] = []\n",
    "    else:\n",
    "        analogies_dict[current_collection].append(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of semantic analogies involving the top 5000 words: 90\n",
      "number of syntactic analogies involving the top 5000 words: 2024\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "sem_analogies = []\n",
    "syn_analogies = []\n",
    "sem_num = 0\n",
    "syn_num = 0\n",
    "\n",
    "for key in analogies_dict.keys():\n",
    "    if 'gram' in key:\n",
    "        lst_choice = syn_analogies\n",
    "        num_choice = syn_num\n",
    "    else:\n",
    "        lst_choice = sem_analogies\n",
    "        num_choice = sem_num\n",
    "\n",
    "    for lst in analogies_dict[key]:\n",
    "        if len(lst) == 4:\n",
    "            flag = True\n",
    "            for i in range(4):\n",
    "                if lst[i] not in top_5k_words_after:\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                lst_choice.append(lst)\n",
    "\n",
    "print(f\"number of semantic analogies involving the top 5000 words: {len(sem_analogies)}\")\n",
    "print(f\"number of syntactic analogies involving the top 5000 words: {len(syn_analogies)}\")\n",
    "\n",
    "with open('sem_analogies.txt', 'w') as f:\n",
    "    f.write(\": sem-tests\\n\")\n",
    "    for i in range(len(sem_analogies)):\n",
    "        f.write(f\"{sem_analogies[i][0]} {sem_analogies[i][1]} {sem_analogies[i][2]} {sem_analogies[i][3]}\\n\") \n",
    "\n",
    "with open('syn_analogies.txt', 'w') as f:\n",
    "    f.write(\": syn-tests\\n\")\n",
    "    for i in range(len(syn_analogies)):\n",
    "        f.write(f\"{syn_analogies[i][0]} {syn_analogies[i][1]} {syn_analogies[i][2]} {syn_analogies[i][3]}\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v accuracy on semantic analogies: 0.9222222222222223%\n",
      "w2v accuracy on syntactic analogies: 0.6773715415019763%\n"
     ]
    }
   ],
   "source": [
    "sem_evaluation = model.evaluate_word_analogies(\"sem_analogies.txt\")[0]\n",
    "syn_evaluation = model.evaluate_word_analogies(\"syn_analogies.txt\")[0]\n",
    "\n",
    "print(f\"w2v accuracy on semantic analogies: {sem_evaluation}%\")\n",
    "print(f\"w2v accuracy on syntactic analogies: {syn_evaluation}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m2300 got 0 out of 90 on the semantic dataset\n"
     ]
    }
   ],
   "source": [
    "m2300_sem_results = []\n",
    "\n",
    "# sem analogies\n",
    "for sem in sem_analogies:\n",
    "    w1, w2, w3, target = sem\n",
    "    \n",
    "    m2300_vec =  M2_300[index_dict[w2]] - M2_300[index_dict[w1]] + M2_300[index_dict[w3]]\n",
    "    #square distance metrix to find closest vector\n",
    "    distances = pairwise_distances(m2300_vec.reshape(1, -1), M2_300, metric=\"l2\").reshape(5030)\n",
    "    ids = distances.argsort()[0]\n",
    "    word = top_5k_words_after[ids]\n",
    "\n",
    "    if word == target:\n",
    "        m2300_sem_results.append(1)\n",
    "    else:\n",
    "        m2300_sem_results.append(0)\n",
    "\n",
    "print(f\"m2300 got {sum(m2300_sem_results)} out of {len(m2300_sem_results)} on the semantic dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m2300 got 0 out of 300 on the semantic dataset\n"
     ]
    }
   ],
   "source": [
    "#syn analogies\n",
    "m2300_syn_results = []\n",
    "\n",
    "# this takes a while to run so please feel free to increase or decrease this amount\n",
    "max_analogies = 300\n",
    "\n",
    "# sem analogies\n",
    "for syn in syn_analogies[:max_analogies]:\n",
    "    w1, w2, w3, target = syn\n",
    "    \n",
    "    m2300_vec =  M2_300[index_dict[w2]] - M2_300[index_dict[w1]] + M2_300[index_dict[w3]]\n",
    "    #square distance metrix to find closest vector\n",
    "    distances = pairwise_distances(m2300_vec.reshape(1, -1), M2_300, metric=\"l2\").reshape(5030)\n",
    "    ids = distances.argsort()[0]\n",
    "    word = top_5k_words_after[ids]\n",
    "\n",
    "    if word == target:\n",
    "        m2300_syn_results.append(1)\n",
    "    else:\n",
    "        m2300_syn_results.append(0)\n",
    "\n",
    "print(f\"m2300 got {sum(m2300_syn_results)} out of {len(m2300_syn_results)} on the semantic dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6014ccf4afa0b85771f484eae679e8248422d3fc19a6d2e10353b8c98042d40e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
